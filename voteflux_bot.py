"""
VoteFlux æ¯æ—¥ç”¢æ¥­æ–°èå ±å‘Š
- çˆ¬å–å„é æ¸¬å¸‚å ´å¹³å°çš„ RSS/éƒ¨è½æ ¼
- ä½¿ç”¨ OpenAI GPT-4o ä»¥è³‡æ·±æ–°èè¨˜è€…è§’åº¦å½™æ•´ 10-15 å‰‡é‡è¦æ–°è
- æ¯å‰‡é™„çŸ­è©•ï¼Œçµå°¾åŠ ç¶œåˆè©•è«–
- ç”¢ç”Ÿ Dark Mode HTML å ±å‘Šéƒ¨ç½²åˆ° GitHub Pages
- æ¨æ’­å ±å‘Šé€£çµåˆ° Telegramï¼ˆå¤šäººæ”¯æ´ï¼‰
"""

import os
import json
import re
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta
from urllib.request import urlopen, Request
from urllib.error import URLError, HTTPError

# â”€â”€â”€ è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TELEGRAM_BOT_TOKEN = os.environ["TELEGRAM_BOT_TOKEN"]
TELEGRAM_CHAT_IDS = [cid.strip() for cid in os.environ["TELEGRAM_CHAT_ID"].split(",")]
OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
GITHUB_PAGES_URL = os.environ.get("GITHUB_PAGES_URL", "https://ä½ çš„å¸³è™Ÿ.github.io/daily-news-bot")

TW_TZ = timezone(timedelta(hours=8))
TODAY = datetime.now(TW_TZ)
TODAY_STR = TODAY.strftime("%Y/%m/%d (%A)")
TODAY_FILE = TODAY.strftime("%Y-%m-%d")


# â”€â”€â”€ å¹³å°è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# å›ºå®š 6 å€‹å¹³å°ï¼ˆå« VoteFluxï¼‰+ 1 å€‹éš¨æ©Ÿç«¶å“ï¼ˆç”± GPT æ±ºå®šï¼‰
FIXED_PLATFORMS = [
    {
        "name": "Polymarket",
        "url": "https://polymarket.com",
        "rss": "https://news.polymarket.com/feed",          # Substack RSS
        "fallback_search": "Polymarket prediction market news",
    },
    {
        "name": "Kalshi",
        "url": "https://kalshi.com",
        "rss": "https://kalshi.com/blog/rss",
        "fallback_search": "Kalshi prediction market news",
    },
    {
        "name": "VoteFlux",
        "url": "https://voteflux.com/en",
        "rss": None,
        "fallback_search": "VoteFlux prediction market news",
    },
    {
        "name": "Hyperliquid",
        "url": "https://hyperliquid.xyz",
        "rss": None,
        "fallback_search": "Hyperliquid DEX news announcement 2025",
    },
    {
        "name": "Predict.fun",
        "url": "https://predict.fun",
        "rss": None,
        "fallback_search": "Predict.fun prediction market news",
    },
]

# DAILY DISCOVERY å€™é¸æ± ï¼ˆçœŸå¯¦å­˜åœ¨çš„å¹³å°ï¼‰
DISCOVERY_CANDIDATES = [
    "Metaculus", "Manifold Markets", "Hedgehog Markets", "PredictIt",
    "Drift Protocol", "Azuro", "PlotX", "Zeitgeist", "Omen", "Futuur",
    "Smarkets", "Betfair Exchange", "Insight Prediction",
    "Iowa Electronic Markets", "Fantasy Top", "Thales Market", "Overtime Markets",
]


# â”€â”€â”€ å·¥å…·å‡½å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_url(url: str, timeout: int = 15) -> str:
    req = Request(url, headers={
        "User-Agent": "Mozilla/5.0 (compatible; VoteFluxBot/2.0)",
        "Accept": "application/rss+xml, application/xml, text/xml, */*",
    })
    with urlopen(req, timeout=timeout) as resp:
        return resp.read().decode("utf-8", errors="replace")


def parse_rss(xml_text: str, max_items: int = 8) -> list[dict]:
    """è§£æ RSS/Atomï¼Œå›å‚³ [{title, link, description, pub_date}]"""
    items = []
    try:
        root = ET.fromstring(xml_text)
    except ET.ParseError:
        return items

    ns = {"atom": "http://www.w3.org/2005/Atom"}

    # RSS 2.0
    for item in root.findall(".//item")[:max_items]:
        title = item.findtext("title", "").strip()
        link = item.findtext("link", "").strip()
        desc = re.sub(r"<[^>]+>", "", item.findtext("description", "")).strip()[:400]
        pub_date = item.findtext("pubDate", "").strip()
        if title:
            items.append({"title": title, "link": link, "description": desc, "pub_date": pub_date})

    # Atom
    if not items:
        for entry in root.findall(".//atom:entry", ns)[:max_items]:
            title = entry.findtext("atom:title", "", ns).strip()
            link_el = entry.find("atom:link", ns)
            link = link_el.get("href", "") if link_el is not None else ""
            desc = re.sub(r"<[^>]+>", "", entry.findtext("atom:summary", "", ns)).strip()[:400]
            pub_date = entry.findtext("atom:updated", "", ns).strip()
            if title:
                items.append({"title": title, "link": link, "description": desc, "pub_date": pub_date})

    return items


def fetch_platform_news(platform: dict) -> dict:
    """å˜—è©¦çˆ¬å–å–®ä¸€å¹³å°çš„ RSSï¼Œå›å‚³çµæœæˆ–ç©ºæ¸…å–®"""
    result = {"name": platform["name"], "url": platform["url"], "articles": [], "source": "none"}

    if platform.get("rss"):
        try:
            xml_text = fetch_url(platform["rss"])
            articles = parse_rss(xml_text)
            if articles:
                result["articles"] = articles
                result["source"] = "rss"
                print(f"  âœ… {platform['name']}: RSS æˆåŠŸï¼Œ{len(articles)} å‰‡")
                return result
        except Exception as e:
            print(f"  âš ï¸ {platform['name']}: RSS å¤±æ•— ({e})")

    # RSS å¤±æ•—æˆ–ç„¡ RSS â†’ æ¨™è¨˜ç‚ºéœ€è¦ GPT è£œå……
    result["source"] = "gpt_needed"
    result["search_hint"] = platform.get("fallback_search", platform["name"] + " news")
    print(f"  â„¹ï¸ {platform['name']}: å°‡ç”± GPT è£œå……è¿‘æ³")
    return result


# â”€â”€â”€ OpenAI API å‘¼å« â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def call_openai(system_prompt: str, user_prompt: str, model: str = "gpt-4o") -> str:
    body = json.dumps({
        "model": model,
        "max_tokens": 4096,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
    }).encode("utf-8")

    req = Request(
        "https://api.openai.com/v1/chat/completions",
        data=body,
        headers={
            "Content-Type": "application/json",
            "Authorization": f"Bearer {OPENAI_API_KEY}",
        },
        method="POST",
    )

    with urlopen(req, timeout=120) as resp:
        data = json.loads(resp.read().decode())

    return data["choices"][0]["message"]["content"]


# â”€â”€â”€ ç”¢ç”Ÿå ±å‘Šè³‡æ–™ï¼ˆJSONï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½æ“æœ‰è¶…é 15 å¹´è³‡æ­·çš„è³‡æ·±è²¡ç¶“ç§‘æŠ€æ–°èè¨˜è€…ï¼Œé•·æœŸæ·±è€•é æ¸¬å¸‚å ´ï¼ˆPrediction Marketï¼‰èˆ‡å»ä¸­å¿ƒåŒ–é‡‘èï¼ˆDeFiï¼‰ç”¢æ¥­å ±å°ã€‚

ä½ çš„èƒŒæ™¯ï¼š
- æ›¾ä»»è·æ–¼ä¸»æµè²¡ç¶“åª’é«”ï¼Œç›®å‰ç¨ç«‹æ’°ç¨¿ï¼Œå°ˆæ³¨é æ¸¬å¸‚å ´ã€äº‹ä»¶åˆç´„ã€éˆä¸Šäº¤æ˜“ç­‰è­°é¡Œ
- ä½ çš„æ–‡ç« é¢¨æ ¼ï¼šå®¢è§€ã€ç²¾æº–ã€æœ‰æ´å¯ŸåŠ›ï¼Œä¸ç‚’ä½œï¼Œä¹Ÿä¸æ‰‹è»Ÿ
- ä½ ç†Ÿæ‚‰ Polymarketã€Kalshiã€Hyperliquid ç­‰ä¸»è¦å¹³å°çš„å•†æ¥­æ¨¡å¼èˆ‡ç›£ç®¡ç’°å¢ƒ
- ä½ é—œæ³¨ç”¢æ¥­è¶¨å‹¢ï¼šæ³•è¦å‹•å‘ã€è³‡é‡‘æµå‘ã€æŠ€è¡“æ¼”é€²ã€ç”¨æˆ¶è¡Œç‚º

ä½ çš„ä»»å‹™æ˜¯æ¯å¤©åŒ¯æ•´é æ¸¬å¸‚å ´ç”¢æ¥­çš„é‡è¦æ–°èï¼Œä»¥è¨˜è€…è¦–è§’æ’°å¯«å ±å‘Šã€‚
æ‰€æœ‰è¼¸å‡ºçš†ä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚ä½ å¿…é ˆä»¥ç´” JSON æ ¼å¼å›è¦†ï¼Œä¸è¦è¼¸å‡ºä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""


def generate_report_data(platform_data: list[dict]) -> dict:
    # æ•´ç†å·²çˆ¬åˆ°çš„å…§å®¹
    crawled_content = ""
    gpt_needed_platforms = []

    for p in platform_data:
        if p["source"] == "rss" and p["articles"]:
            crawled_content += f"\n\n### {p['name']} (ä¾†æº: RSS)\n"
            for a in p["articles"]:
                crawled_content += f"- æ¨™é¡Œ: {a['title']}\n"
                if a.get("description"):
                    crawled_content += f"  æ‘˜è¦: {a['description'][:200]}\n"
                if a.get("pub_date"):
                    crawled_content += f"  æ—¥æœŸ: {a['pub_date']}\n"
        else:
            gpt_needed_platforms.append(p["name"])

    gpt_needed_str = ""
    if gpt_needed_platforms:
        gpt_needed_str = f"""
ä»¥ä¸‹å¹³å°æœªèƒ½çˆ¬å– RSSï¼Œè«‹æ ¹æ“šä½ çš„çŸ¥è­˜è£œå……é€™äº›å¹³å°æˆªè‡³ä»Šæ—¥çš„è¿‘æœŸé‡è¦å‹•æ…‹ï¼ˆæœ€è¿‘ 2-4 é€±å…§çš„çœŸå¯¦äº‹ä»¶ï¼Œè‹¥ä¸ç¢ºå®šè«‹ä¸è¦æé€ ï¼‰ï¼š
{', '.join(gpt_needed_platforms)}
"""

    # Daily Discovery å¹³å°ç”± GPT å¾å€™é¸æ± æŒ‘é¸
    candidates_str = ", ".join(DISCOVERY_CANDIDATES)

    user_prompt = f"""ä»Šå¤©æ˜¯ {TODAY_STR}ã€‚

ä½ éœ€è¦å½™æ•´ä¸€ä»½é æ¸¬å¸‚å ´ç”¢æ¥­æ¯æ—¥æ–°èå ±å‘Šã€‚

ã€å·²çˆ¬å–çš„å¹³å°æ–°èã€‘
{crawled_content if crawled_content else "ï¼ˆæœ¬æ¬¡æœªèƒ½çˆ¬å–åˆ° RSS å…§å®¹ï¼‰"}

{gpt_needed_str}

ã€DAILY DISCOVERYã€‘
å¾ä»¥ä¸‹çœŸå¯¦å­˜åœ¨çš„å¹³å°å€™é¸æ± ä¸­æŒ‘é¸ä»Šå¤©çš„ 1 å€‹é‡é»å¹³å°é€²è¡Œä»‹ç´¹ï¼ˆä¸èƒ½é‡è¤‡é¸å›ºå®šçš„ 6 å€‹å¹³å°ï¼‰ï¼š
å€™é¸å¹³å°ï¼š{candidates_str}

âš ï¸ é¸çš„å¹³å°å¿…é ˆæ˜¯çœŸå¯¦å­˜åœ¨ä¸”ç›®å‰ä»åœ¨é‹ç‡Ÿçš„ï¼Œç¶²å€å¿…é ˆçœŸå¯¦å¯é€£ç·šã€‚

ã€ä»»å‹™èªªæ˜ã€‘
1. æ•´åˆä¸Šè¿°æ‰€æœ‰å¹³å°çš„æ–°èï¼Œå¾ä¸­æŒ‘å‡ºä»Šå¤©æœ€å€¼å¾—é—œæ³¨çš„ 10-15 å‰‡æ–°èï¼ˆæ¶µè“‹å¤šå€‹å¹³å°ï¼‰
2. æ¯å‰‡æ–°èé™„ä¸€å¥è©±è¨˜è€…çŸ­è©•ï¼ˆå®¢è§€ã€æœ‰æ´å¯ŸåŠ›ã€ä¸è¶…é 50 å­—ï¼‰
3. åœ¨æ‰€æœ‰æ–°èçµæŸå¾Œï¼Œæ’°å¯«ä¸€æ®µã€Œä»Šæ—¥ç”¢æ¥­ç¶œåˆè©•è«–ã€ï¼ˆ300-500 å­—ï¼Œè¨˜è€…ç¬¬ä¸€äººç¨±ï¼Œåˆ†ææ•´é«”è¶¨å‹¢ï¼‰
4. é¸å‡ºä»Šæ—¥ DAILY DISCOVERY å¹³å°

è«‹ä»¥åš´æ ¼ JSON æ ¼å¼å›è¦†ï¼ˆä¸è¦åŠ  markdown ä»£ç¢¼å¡Šï¼‰ï¼Œçµæ§‹å¦‚ä¸‹ï¼š

{{
  "daily_discovery": {{
    "name": "å¹³å°åç¨±",
    "url": "çœŸå¯¦ç¶²å€",
    "category": "å¹³å°é¡å‹ï¼ˆå¦‚ï¼šç¤¾ç¾¤é æ¸¬ã€åˆè¦äº¤æ˜“æ‰€ã€DeFi ç­‰ï¼‰",
    "description": "é€™å¹³å°åšä»€éº¼ï¼ˆ2-3å¥ï¼‰",
    "reporter_note": "è¨˜è€…è¦–è§’çš„è§€å¯Ÿï¼ˆ2-3å¥ï¼Œåˆ†æå…¶åœ¨ç”¢æ¥­ä¸­çš„å®šä½ï¼‰"
  }},
  "news_items": [
    {{
      "id": 1,
      "platform": "å¹³å°åç¨±",
      "title": "æ–°èæ¨™é¡Œ",
      "summary": "æ–°èæ‘˜è¦ï¼ˆ2-3å¥ï¼Œå®¢è§€æè¿°äº‹ä»¶ï¼‰",
      "reporter_comment": "è¨˜è€…çŸ­è©•ï¼ˆä¸€å¥è©±ï¼Œæœ‰æ´å¯ŸåŠ›ï¼‰",
      "source_url": "åŸæ–‡å®Œæ•´ URLï¼ˆå¿…é ˆä»¥ https:// é–‹é ­ï¼›è‹¥ä¸ç¢ºå®šè«‹å¡«ç©ºå­—ä¸² \"\"ï¼Œçµ•å°ä¸è¦å¡« example.com æˆ–å‡ç¶²å€ï¼‰",
      "importance": "high/medium/low"
    }}
  ],
  "industry_analysis": {{
    "headline": "ä»Šæ—¥åˆ†ææ¨™é¡Œï¼ˆä¸€å¥è©±ç ´é¡Œï¼‰",
    "content": "ä»Šæ—¥ç”¢æ¥­ç¶œåˆè©•è«–å…¨æ–‡ï¼ˆ300-500å­—ï¼Œç¹é«”ä¸­æ–‡ï¼Œè¨˜è€…ç¬¬ä¸€äººç¨±ï¼‰",
    "key_trends": ["è¶¨å‹¢é—œéµå­—1", "è¶¨å‹¢é—œéµå­—2", "è¶¨å‹¢é—œéµå­—3"]
  }}
}}

news_items å¿…é ˆåŒ…å« 10-15 å‰‡ï¼Œimportance æ¬„ä½ç”¨æ–¼æ’ç‰ˆå„ªå…ˆç´šã€‚
åªè¼¸å‡º JSONï¼Œä¸è¦è¼¸å‡ºä»»ä½•å…¶ä»–æ–‡å­—ã€‚"""

    raw = call_openai(SYSTEM_PROMPT, user_prompt)

    # æ¸…ç† markdown åŒ…è£¹
    raw = re.sub(r'^```json?\s*\n?', '', raw.strip())
    raw = re.sub(r'\n?```\s*$', '', raw.strip())

    print(f"ğŸ” [DEBUG] JSON é•·åº¦: {len(raw)} å­—å…ƒ")
    print(f"ğŸ” [DEBUG] å‰ 200 å­—:\n{raw[:200]}")

    return json.loads(raw)


# â”€â”€â”€ çµ„è£ HTML â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PLATFORM_COLORS = {
    "Polymarket": "#0066ff",
    "Kalshi": "#00b386",
    "VoteFlux": "#f0883e",
    "Hyperliquid": "#8957e5",
    "Predict.fun": "#d29922",
}

IMPORTANCE_LABELS = {
    "high": ("ğŸ”´", "é‡é»"),
    "medium": ("ğŸŸ¡", "ä¸€èˆ¬"),
    "low": ("âšª", "åƒè€ƒ"),
}


def get_platform_color(name: str) -> str:
    for key, color in PLATFORM_COLORS.items():
        if key.lower() in name.lower():
            return color
    return "#58a6ff"


def build_html(data: dict) -> str:
    dd = data["daily_discovery"]
    news_items = data["news_items"]
    analysis = data["industry_analysis"]

    # ä¾ importance æ’åºï¼šhigh â†’ medium â†’ low
    importance_order = {"high": 0, "medium": 1, "low": 2}
    sorted_news = sorted(news_items, key=lambda x: importance_order.get(x.get("importance", "medium"), 1))

    # çµ±è¨ˆå„å¹³å°æ–°èæ•¸é‡
    platform_counts: dict[str, int] = {}
    for item in news_items:
        p = item.get("platform", "å…¶ä»–")
        platform_counts[p] = platform_counts.get(p, 0) + 1

    platform_pills = ""
    for p, count in sorted(platform_counts.items(), key=lambda x: -x[1]):
        color = get_platform_color(p)
        platform_pills += f'<span class="platform-pill" style="border-color:{color};color:{color}">{p} <b>{count}</b></span>'

    # è¶¨å‹¢æ¨™ç±¤
    trend_tags = "".join(f'<span class="trend-tag">{t}</span>' for t in analysis.get("key_trends", []))

    # æ–°èå¡ç‰‡
    news_cards = ""
    for item in sorted_news:
        imp = item.get("importance", "medium")
        imp_icon, imp_label = IMPORTANCE_LABELS.get(imp, ("âšª", "åƒè€ƒ"))
        color = get_platform_color(item.get("platform", ""))
        source_link = ""
        raw_url = item.get("source_url", "") or ""
        if raw_url.startswith("http://") or raw_url.startswith("https://"):
            source_link = f'<a href="{raw_url}" target="_blank" class="source-link">åŸæ–‡ â†’</a>'

        news_cards += f"""
        <div class="news-card importance-{imp}">
            <div class="news-header">
                <span class="platform-badge" style="background:rgba({hex_to_rgb(color)},0.15);color:{color};border:1px solid {color}">{item.get('platform','')}</span>
                <span class="importance-badge">{imp_icon} {imp_label}</span>
                {source_link}
            </div>
            <div class="news-title">{item.get('title','')}</div>
            <div class="news-summary">{item.get('summary','')}</div>
            <div class="reporter-comment">
                <span class="comment-icon">ğŸ–Š</span>
                <span class="comment-text">{item.get('reporter_comment','')}</span>
            </div>
        </div>"""

    return f"""<!DOCTYPE html>
<html lang="zh-TW">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>VoteFlux ç”¢æ¥­æ–°èæ—¥å ± â€” {TODAY_STR}</title>
<style>
    * {{ margin: 0; padding: 0; box-sizing: border-box; }}
    body {{
        background: #0d1117; color: #c9d1d9;
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Noto Sans TC', sans-serif;
        line-height: 1.7; padding: 20px; max-width: 960px; margin: 0 auto;
    }}
    a {{ color: #58a6ff; text-decoration: none; }}
    a:hover {{ text-decoration: underline; }}

    /* Header */
    .header {{
        text-align: center; padding: 36px 0 28px;
        border-bottom: 2px solid #21262d; margin-bottom: 32px;
    }}
    .header h1 {{ color: #e6edf3; font-size: 1.9em; font-weight: 700; letter-spacing: -0.5px; }}
    .header .subtitle {{ color: #8b949e; font-size: 0.95em; margin-top: 6px; }}
    .header .date {{ color: #f0883e; font-size: 1em; margin-top: 10px; font-weight: 600; }}

    /* Section titles */
    h2 {{
        color: #e6edf3; font-size: 1.2em; font-weight: 700;
        margin: 36px 0 16px;
        display: flex; align-items: center; gap: 10px;
    }}
    h2::after {{
        content: ''; flex: 1; height: 1px; background: #21262d;
    }}

    /* Platform pills */
    .platform-summary {{
        display: flex; flex-wrap: wrap; gap: 8px; margin-bottom: 24px;
    }}
    .platform-pill {{
        padding: 4px 12px; border-radius: 20px; border: 1px solid;
        font-size: 0.85em; font-weight: 500;
    }}
    .platform-pill b {{ font-weight: 700; }}

    /* Daily Discovery */
    .discovery {{
        background: linear-gradient(135deg, #161b22, #1a2030);
        border: 1px solid #f0883e; border-radius: 12px;
        padding: 24px; margin-bottom: 8px;
    }}
    .discovery-meta {{
        display: flex; align-items: center; gap: 10px; margin-bottom: 14px; flex-wrap: wrap;
    }}
    .discovery-badge {{
        background: #f0883e; color: #0d1117;
        padding: 3px 12px; border-radius: 20px;
        font-size: 0.8em; font-weight: 700; letter-spacing: 1px;
    }}
    .discovery-name {{ color: #f0883e; font-size: 1.3em; font-weight: 700; }}
    .discovery-category {{
        background: rgba(240,136,62,0.1); color: #f0883e;
        padding: 2px 10px; border-radius: 4px; font-size: 0.8em;
    }}
    .discovery-url {{ font-size: 0.85em; margin-bottom: 12px; }}
    .discovery p {{ color: #c9d1d9; margin-bottom: 12px; font-size: 0.95em; }}
    .reporter-note-box {{
        background: rgba(240,136,62,0.07); border-left: 3px solid #f0883e;
        padding: 12px 16px; border-radius: 0 6px 6px 0;
        font-size: 0.9em; color: #e6edf3;
    }}
    .reporter-note-box::before {{
        content: "è¨˜è€…è§€å¯Ÿ â”€â”€ "; font-weight: 700; color: #f0883e;
    }}

    /* News cards */
    .news-list {{ display: flex; flex-direction: column; gap: 12px; }}
    .news-card {{
        background: #161b22; border: 1px solid #21262d;
        border-radius: 10px; padding: 18px 20px;
        transition: border-color 0.2s;
    }}
    .news-card:hover {{ border-color: #30363d; }}
    .news-card.importance-high {{ border-left: 3px solid #f85149; }}
    .news-card.importance-medium {{ border-left: 3px solid #d29922; }}
    .news-card.importance-low {{ border-left: 3px solid #30363d; }}

    .news-header {{
        display: flex; align-items: center; gap: 8px;
        margin-bottom: 10px; flex-wrap: wrap;
    }}
    .platform-badge {{
        padding: 2px 10px; border-radius: 4px;
        font-size: 0.78em; font-weight: 600;
    }}
    .importance-badge {{
        font-size: 0.78em; color: #8b949e;
    }}
    .source-link {{
        margin-left: auto; font-size: 0.8em; color: #58a6ff;
    }}
    .news-title {{
        font-size: 1.0em; font-weight: 600; color: #e6edf3;
        margin-bottom: 8px; line-height: 1.5;
    }}
    .news-summary {{
        font-size: 0.88em; color: #8b949e; margin-bottom: 10px;
        line-height: 1.6;
    }}
    .reporter-comment {{
        display: flex; gap: 8px; align-items: flex-start;
        background: rgba(88,166,255,0.05); border-radius: 6px;
        padding: 8px 12px;
    }}
    .comment-icon {{ flex-shrink: 0; margin-top: 1px; }}
    .comment-text {{
        font-size: 0.88em; color: #79c0ff; font-style: italic;
        line-height: 1.5;
    }}

    /* Industry Analysis */
    .analysis-box {{
        background: linear-gradient(135deg, #161b22, #1a2030);
        border: 1px solid #21262d; border-radius: 12px; padding: 28px;
        margin-top: 8px;
    }}
    .analysis-headline {{
        font-size: 1.15em; font-weight: 700; color: #58a6ff;
        margin-bottom: 16px; line-height: 1.4;
    }}
    .analysis-content {{
        font-size: 0.95em; color: #c9d1d9; line-height: 1.9;
        white-space: pre-line;
    }}
    .trend-tags {{ display: flex; gap: 8px; flex-wrap: wrap; margin-top: 20px; }}
    .trend-tag {{
        background: rgba(88,166,255,0.1); color: #58a6ff;
        padding: 4px 12px; border-radius: 20px; font-size: 0.82em;
        border: 1px solid rgba(88,166,255,0.3);
    }}

    /* Footer */
    .footer {{
        text-align: center; margin-top: 50px; padding-top: 20px;
        border-top: 1px solid #21262d; color: #484f58; font-size: 0.82em;
    }}

    @media (max-width: 640px) {{
        body {{ padding: 12px; }}
        .header h1 {{ font-size: 1.5em; }}
        .source-link {{ margin-left: 0; }}
    }}
</style>
</head>
<body>

<div class="header">
    <h1>ğŸ“° VoteFlux ç”¢æ¥­æ–°èæ—¥å ±</h1>
    <div class="subtitle">PREDICTION MARKET DAILY BRIEFING</div>
    <div class="date">{TODAY_STR}</div>
</div>

<!-- å¹³å°åˆ†ä½ˆ -->
<div class="platform-summary">
    {platform_pills}
</div>

<!-- Daily Discovery -->
<h2>ğŸ” ä»Šæ—¥å¹³å°èšç„¦</h2>
<div class="discovery">
    <div class="discovery-meta">
        <span class="discovery-badge">DAILY DISCOVERY</span>
        <span class="discovery-name">{dd['name']}</span>
        <span class="discovery-category">{dd.get('category','')}</span>
    </div>
    <div class="discovery-url"><a href="{dd.get('url','')}" target="_blank">{dd.get('url','')}</a></div>
    <p>{dd['description']}</p>
    <div class="reporter-note-box">{dd['reporter_note']}</div>
</div>

<!-- æ–°èåˆ—è¡¨ -->
<h2>ğŸ“‹ ä»Šæ—¥é‡è¦æ–°èï¼ˆ{len(sorted_news)} å‰‡ï¼‰</h2>
<div class="news-list">
    {news_cards}
</div>

<!-- ç¶œåˆè©•è«– -->
<h2>ğŸ“ ä»Šæ—¥ç”¢æ¥­ç¶œåˆè©•è«–</h2>
<div class="analysis-box">
    <div class="analysis-headline">"{analysis['headline']}"</div>
    <div class="analysis-content">{analysis['content']}</div>
    <div class="trend-tags">{trend_tags}</div>
</div>

<div class="footer">
    <p>Â© 2026 VoteFlux Daily Intelligence | Generated by AI | æœ¬å ±å‘Šåƒ…ä¾›åƒè€ƒï¼Œä¸æ§‹æˆä»»ä½•æŠ•è³‡å»ºè­°</p>
</div>

</body>
</html>"""


def hex_to_rgb(hex_color: str) -> str:
    """#rrggbb â†’ 'r,g,b'"""
    hex_color = hex_color.lstrip("#")
    if len(hex_color) == 6:
        r, g, b = int(hex_color[0:2], 16), int(hex_color[2:4], 16), int(hex_color[4:6], 16)
        return f"{r},{g},{b}"
    return "88,166,255"


# â”€â”€â”€ æª”æ¡ˆå„²å­˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_html_report(html_content: str) -> str:
    os.makedirs("reports", exist_ok=True)

    filename = f"reports/voteflux-{TODAY_FILE}.html"
    with open(filename, "w", encoding="utf-8") as f:
        f.write(html_content)

    with open("reports/index.html", "w", encoding="utf-8") as f:
        f.write(f"""<!DOCTYPE html>
<html><head>
<meta charset="UTF-8">
<meta http-equiv="refresh" content="0; url=voteflux-{TODAY_FILE}.html">
<title>VoteFlux æœ€æ–°æ—¥å ±</title>
</head><body>
<p>æ­£åœ¨è·³è½‰åˆ°æœ€æ–°å ±å‘Š... <a href="voteflux-{TODAY_FILE}.html">é»æ­¤å‰å¾€</a></p>
</body></html>""")

    print(f"ğŸ“„ å ±å‘Šå·²å„²å­˜: {filename}")
    return filename


# â”€â”€â”€ Telegram å¤šäººæ¨æ’­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def send_telegram(text: str):
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"

    for chat_id in TELEGRAM_CHAT_IDS:
        body = json.dumps({
            "chat_id": chat_id,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": False,
        }).encode("utf-8")

        req = Request(url, data=body, headers={"Content-Type": "application/json"}, method="POST")

        try:
            with urlopen(req, timeout=15) as resp:
                result = json.loads(resp.read().decode())
                if not result.get("ok"):
                    print(f"âš ï¸ Telegram ç™¼é€å¤±æ•— (chat_id: {chat_id}): {result}")
                else:
                    print(f"âœ… è¨Šæ¯å·²ç™¼é€åˆ° {chat_id}")
        except Exception as e:
            # fallback ç´”æ–‡å­—
            print(f"âš ï¸ HTML æ ¼å¼å¤±æ•—ï¼Œå˜—è©¦ç´”æ–‡å­— (chat_id: {chat_id}): {e}")
            try:
                plain = re.sub(r"<[^>]+>", "", text)
                body2 = json.dumps({"chat_id": chat_id, "text": plain}).encode("utf-8")
                req2 = Request(url, data=body2, headers={"Content-Type": "application/json"}, method="POST")
                with urlopen(req2, timeout=15) as resp2:
                    print(f"âœ… ç´”æ–‡å­—å·²ç™¼é€åˆ° {chat_id}")
            except Exception as e2:
                print(f"âŒ å®Œå…¨å¤±æ•— (chat_id: {chat_id}): {e2}")


# â”€â”€â”€ ä¸»ç¨‹å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("=" * 55)
    print(f"ğŸ“° VoteFlux ç”¢æ¥­æ–°èæ—¥å ± â€” {TODAY_STR}")
    print("=" * 55)

    # Step 1: çˆ¬å–å„å¹³å° RSS
    print("\nğŸ“¡ æ­£åœ¨çˆ¬å–å„å¹³å°æ–°èä¾†æº...")
    platform_data = []
    for platform in FIXED_PLATFORMS:
        result = fetch_platform_news(platform)
        platform_data.append(result)

    rss_success = sum(1 for p in platform_data if p["source"] == "rss")
    print(f"ğŸ“Š RSS çˆ¬å–æˆåŠŸ: {rss_success}/{len(FIXED_PLATFORMS)} å€‹å¹³å°")

    # Step 2: GPT-4o å½™æ•´æ–°è + ç”¢ç”Ÿå ±å‘Šè³‡æ–™
    print("\nğŸ¤– æ­£åœ¨ç”¨ GPT-4o å½™æ•´æ–°èä¸¦ç”¢ç”Ÿå ±å‘Šè³‡æ–™...")
    try:
        report_data = generate_report_data(platform_data)
        news_count = len(report_data.get("news_items", []))
        print(f"âœ… JSON è§£ææˆåŠŸï¼Œå…± {news_count} å‰‡æ–°è")
    except (json.JSONDecodeError, KeyError, IndexError) as e:
        print(f"âŒ JSON è§£æå¤±æ•—: {e}")
        send_telegram(f"âš ï¸ <b>VoteFlux ç”¢æ¥­æ–°èæ—¥å ± â€” {TODAY_STR}</b>\n\nå ±å‘Šç”¢ç”Ÿå¤±æ•—ï¼Œè«‹æ‰‹å‹•æª¢æŸ¥ Action logã€‚")
        return

    # Step 3: çµ„è£ HTML
    print("\nğŸ”¨ æ­£åœ¨çµ„è£ HTML å ±å‘Š...")
    html_content = build_html(report_data)
    save_html_report(html_content)

    # Step 4: æ¨æ’­åˆ° Telegram
    news_count = len(report_data.get("news_items", []))
    discovery_name = report_data.get("daily_discovery", {}).get("name", "")
    report_url = f"{GITHUB_PAGES_URL}/voteflux-{TODAY_FILE}.html"

    message = (
        f"ğŸ“° <b>VoteFlux ç”¢æ¥­æ–°èæ—¥å ± â€” {TODAY_STR}</b>\n\n"
        f"ä»Šæ—¥å½™æ•´ <b>{news_count} å‰‡</b>é‡è¦æ–°è\n"
        f"ğŸ” ä»Šæ—¥èšç„¦ï¼š<b>{discovery_name}</b>\n\n"
        f"ğŸ”— <a href=\"{report_url}\">ğŸ“– æŸ¥çœ‹å®Œæ•´å ±å‘Š</a>"
    )

    print("\nğŸ“¤ æ­£åœ¨æ¨æ’­åˆ° Telegram...")
    send_telegram(message)

    print("\nğŸ‰ VoteFlux ç”¢æ¥­æ–°èæ—¥å ±å®Œæˆï¼")


if __name__ == "__main__":
    main()
