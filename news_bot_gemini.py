"""
æ¯æ—¥æ–°èæ‘˜è¦ Telegram Bot
- æŠ“å–å°ç£ç¶œåˆã€åœ‹éš›ã€ç§‘æŠ€ã€è²¡ç¶“æ–°è (RSS)
- ä½¿ç”¨ Google Gemini API (Gemini 1.5 Flash) ç”¢ç”Ÿä¸­æ–‡æ‘˜è¦ â€” å®Œå…¨å…è²»
- æ¨æ’­åˆ° Telegram
"""

import os
import json
import re
import hashlib
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta
from email.utils import parsedate_to_datetime
from urllib.request import urlopen, Request
from urllib.parse import quote

# â”€â”€â”€ è¨­å®š â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TELEGRAM_BOT_TOKEN = os.environ["TELEGRAM_BOT_TOKEN"]
TELEGRAM_CHAT_IDS = [cid.strip() for cid in os.environ["TELEGRAM_CHAT_ID"].split(",")]
GEMINI_API_KEY = os.environ["GEMINI_API_KEY"]

# å°ç£æ™‚é–“
TW_TZ = timezone(timedelta(hours=8))

# æ‰‹å‹•è§¸ç™¼æ¨¡å¼ï¼ˆworkflow_dispatchï¼‰â†’ ä¸åšå»é‡è¤‡ï¼Œæ–¹ä¾¿æ¸¬è©¦
IS_MANUAL = os.environ.get("IS_MANUAL", "false").lower() == "true"

# â”€â”€â”€ RSS æ–°èä¾†æº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RSS_FEEDS = {
    "ğŸ‡¹ğŸ‡¼ å°ç£ç¶œåˆ": [
        "https://news.ltn.com.tw/rss/all.xml",
        "https://feeds.feedburner.com/ettoday/global",
    ],
    "ğŸŒ åœ‹éš›æ–°è": [
        "https://news.ltn.com.tw/rss/world.xml",
        "https://www.cna.com.tw/rss/aall.xml",
    ],
    "ğŸ’» ç§‘æŠ€æ–°è": [
        "https://feeds.feedburner.com/ithome",
        "https://technews.tw/feed/",
    ],
    "ğŸ¤– AI æ–°è": [
        "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",  # The Verge AI
        "https://venturebeat.com/ai/feed/",                                    # VentureBeat AI
        "https://techcrunch.com/tag/artificial-intelligence/feed/",            # TechCrunch AI
    ],
    "ğŸ’° è²¡ç¶“æ–°è": [
        "https://news.ltn.com.tw/rss/business.xml",
        "https://www.cna.com.tw/rss/aafe.xml",
    ],
}

MAX_ITEMS_PER_FEED = 20  # å¤šæŠ“ä¸€äº›ï¼Œéæ¿¾æ—¥æœŸå¾Œå†é™åˆ¶æ•¸é‡
MAX_ITEMS_PER_FEED_FINAL = 5  # éæ¿¾å¾Œæ¯å€‹ä¾†æºæœ€å¤šä¿ç•™å¹¾å‰‡

# â”€â”€â”€ é»‘åå–®ï¼ˆæ¨™é¡Œå«é€™äº›é—œéµå­—çš„æ–°èç›´æ¥è·³éï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TITLE_BLACKLIST = [
    "å†°èˆ‡ç«ä¹‹æ­Œ",
    "æ¬ŠåŠ›éŠæˆ²",
]

SEEN_FILE = "seen_titles.json"  # ç”± GitHub Actions Cache è·¨å¤©ä¿ç•™


# â”€â”€â”€ å»é‡è¤‡æ©Ÿåˆ¶ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def title_hash(title: str) -> str:
    """å°‡æ¨™é¡Œè½‰æˆçŸ­ hashï¼Œé¿å…å­˜å¤ªé•·çš„å­—ä¸²"""
    return hashlib.md5(title.strip().encode("utf-8")).hexdigest()


def load_seen() -> set:
    """è®€å–å·²æ¨æ’­éçš„æ¨™é¡Œ hash"""
    if os.path.exists(SEEN_FILE):
        try:
            with open(SEEN_FILE, "r") as f:
                return set(json.load(f))
        except Exception:
            pass
    return set()


def save_seen(seen: set):
    """å„²å­˜å·²æ¨æ’­çš„æ¨™é¡Œ hashï¼ˆåªä¿ç•™æœ€è¿‘ 500 ç­†ï¼Œé¿å…ç„¡é™è†¨è„¹ï¼‰"""
    seen_list = list(seen)[-500:]
    with open(SEEN_FILE, "w") as f:
        json.dump(seen_list, f)


def filter_seen(items: list[dict], seen: set) -> list[dict]:
    """éæ¿¾æ‰å·²æ¨æ’­éçš„æ–°è"""
    return [item for item in items if title_hash(item["title"]) not in seen]


def mark_seen(items: list[dict], seen: set):
    """å°‡æœ¬æ¬¡æ¨æ’­çš„æ¨™é¡ŒåŠ å…¥ seen"""
    for item in items:
        seen.add(title_hash(item["title"]))


def is_today(pub_date_str: str) -> bool:
    """åˆ¤æ–·ç™¼å¸ƒæ—¥æœŸæ˜¯å¦ç‚ºä»Šå¤©ï¼ˆå°ç£æ™‚é–“ï¼‰ã€‚ç„¡æ³•è§£ææ™‚å›å‚³ Trueï¼ˆä¿ç•™è©²å‰‡æ–°èï¼‰ã€‚"""
    if not pub_date_str:
        return True
    try:
        # RSS 2.0 çš„ pubDate æ ¼å¼ï¼šRFC 2822ï¼Œä¾‹å¦‚ "Mon, 24 Feb 2026 01:00:00 +0800"
        pub_dt = parsedate_to_datetime(pub_date_str)
        pub_tw = pub_dt.astimezone(TW_TZ)
        today_tw = datetime.now(TW_TZ).date()
        return pub_tw.date() == today_tw
    except Exception:
        return True  # è§£æå¤±æ•—æ™‚ä¿ç•™ï¼Œä¸èª¤æ®º


# â”€â”€â”€ å·¥å…·å‡½å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_url(url: str, timeout: int = 15) -> str:
    """å–å¾—ç¶²é å…§å®¹"""
    req = Request(url, headers={"User-Agent": "Mozilla/5.0 DailyNewsBot/1.0"})
    with urlopen(req, timeout=timeout) as resp:
        return resp.read().decode("utf-8", errors="replace")


def parse_rss(xml_text: str, max_items: int = MAX_ITEMS_PER_FEED, skip_date_filter: bool = False) -> list[dict]:
    """è§£æ RSS/Atom feedï¼Œå›å‚³ [{title, link, description}]ã€‚skip_date_filter=True æ™‚ä¸éæ¿¾æ—¥æœŸï¼ˆç”¨æ–¼è‹±æ–‡ä¾†æºï¼‰"""
    items = []
    try:
        root = ET.fromstring(xml_text)
    except ET.ParseError:
        return items

    # è™•ç†ä¸åŒçš„ namespace
    ns = {"atom": "http://www.w3.org/2005/Atom"}

    # RSS 2.0
    for item in root.findall(".//item")[:max_items]:
        pub_date = item.findtext("pubDate", "").strip()
        if not skip_date_filter and not is_today(pub_date):
            continue  # â† è·³ééä»Šå¤©çš„æ–°è

        title = item.findtext("title", "").strip()
        if any(kw in title for kw in TITLE_BLACKLIST):
            continue  # â† é»‘åå–®éæ¿¾
        link = item.findtext("link", "").strip()
        desc = item.findtext("description", "").strip()
        desc = re.sub(r"<[^>]+>", "", desc)[:300]
        if title:
            items.append({"title": title, "link": link, "description": desc})

    # Atom feed
    if not items:
        for entry in root.findall(".//atom:entry", ns)[:max_items]:
            # Atom çš„æ—¥æœŸæ¬„ä½æ˜¯ <updated> æˆ– <published>
            pub_date = (
                entry.findtext("atom:published", "", ns)
                or entry.findtext("atom:updated", "", ns)
            ).strip()

            # Atom æ—¥æœŸæ ¼å¼æ˜¯ ISO 8601ï¼Œéœ€è¦å¦å¤–è§£æ
            if pub_date and not skip_date_filter:
                try:
                    pub_dt = datetime.fromisoformat(pub_date.replace("Z", "+00:00"))
                    pub_tw = pub_dt.astimezone(TW_TZ)
                    today_tw = datetime.now(TW_TZ).date()
                    if pub_tw.date() != today_tw:
                        continue  # â† è·³ééä»Šå¤©çš„æ–°è
                except Exception:
                    pass  # è§£æå¤±æ•—å°±ä¿ç•™

            title = entry.findtext("atom:title", "", ns).strip()
            if any(kw in title for kw in TITLE_BLACKLIST):
                continue  # â† é»‘åå–®éæ¿¾
            link_el = entry.find("atom:link", ns)
            link = link_el.get("href", "") if link_el is not None else ""
            desc = entry.findtext("atom:summary", "", ns).strip()
            desc = re.sub(r"<[^>]+>", "", desc)[:300]
            if title:
                items.append({"title": title, "link": link, "description": desc})

    return items[:MAX_ITEMS_PER_FEED_FINAL]


# è‹±æ–‡ä¾†æºä¸åšæ—¥æœŸéæ¿¾ï¼ˆå› ç‚ºç¾åœ‹æ™‚é–“æ¯”å°ç£æ™šï¼Œæ—©ä¸Šè·‘æ™‚æ–‡ç« æ—¥æœŸé‚„æ˜¯æ˜¨å¤©ï¼‰
EN_FEEDS = {
    "https://www.theverge.com/ai-artificial-intelligence/rss/index.xml",
    "https://venturebeat.com/ai/feed/",
    "https://techcrunch.com/tag/artificial-intelligence/feed/",
}


def fetch_all_news(seen: set) -> dict[str, list[dict]]:
    """æŠ“å–æ‰€æœ‰åˆ†é¡çš„æ–°èï¼Œä¸¦éæ¿¾å·²æ¨æ’­éçš„æ¨™é¡Œ"""
    all_news = {}
    for category, feeds in RSS_FEEDS.items():
        category_items = []
        for feed_url in feeds:
            try:
                xml_text = fetch_url(feed_url)
                skip_date = feed_url in EN_FEEDS  # è‹±æ–‡ä¾†æºä¸åšæ—¥æœŸéæ¿¾
                fetched = parse_rss(xml_text, skip_date_filter=skip_date)
                if not IS_MANUAL:
                    fetched = filter_seen(fetched, seen)  # â† è‡ªå‹•æ’ç¨‹æ‰å»é‡è¤‡
                print(f"  ğŸ“Œ {feed_url} â†’ {len(fetched)} å‰‡")
                category_items.extend(fetched)
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•æŠ“å– {feed_url}: {e}")
        all_news[category] = category_items
    return all_news


def build_prompt(all_news: dict[str, list[dict]]) -> str:
    """çµ„åˆ prompt çµ¦ GPT åšæ‘˜è¦"""
    news_text = ""
    for category, items in all_news.items():
        news_text += f"\n\n## {category}\n"
        for i, item in enumerate(items, 1):
            news_text += f"{i}. {item['title']}\n"
            if item["description"]:
                news_text += f"   {item['description']}\n"

    today = datetime.now(TW_TZ).strftime("%Y/%m/%d (%A)")

    return f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ–°èç·¨è¼¯ã€‚ä»¥ä¸‹æ˜¯ä»Šå¤©ï¼ˆ{today}ï¼‰å¾å„å¤§åª’é«”æŠ“å–çš„æ–°èæ¨™é¡Œèˆ‡æ‘˜è¦ã€‚

è«‹å¹«æˆ‘ï¼š
1. æ¯å€‹åˆ†é¡æŒ‘å‡º 3-5 å‰‡æœ€é‡è¦çš„æ–°è
2. ç”¨ç¹é«”ä¸­æ–‡æ’°å¯«ç°¡çŸ­æ‘˜è¦ï¼ˆæ¯å‰‡ 1-2 å¥è©±ï¼‰
3. æ ¼å¼ä½¿ç”¨ Telegram æ”¯æ´çš„ HTML æ ¼å¼

è¼¸å‡ºæ ¼å¼ç¯„ä¾‹ï¼š
<b>ğŸ“° æ¯æ—¥æ–°èæ‘˜è¦ â€” {today}</b>

<b>ğŸ‡¹ğŸ‡¼ å°ç£ç¶œåˆ</b>
â€¢ <b>æ¨™é¡Œ</b>ï¼šä¸€å¥è©±æ‘˜è¦
â€¢ <b>æ¨™é¡Œ</b>ï¼šä¸€å¥è©±æ‘˜è¦

ï¼ˆå…¶ä»–åˆ†é¡åŒä¸Šï¼‰

çµå°¾åŠ ä¸Šä¸€å¥é¼“å‹µçš„è©±ã€‚

ä»¥ä¸‹æ˜¯ä»Šå¤©çš„åŸå§‹æ–°èï¼š
{news_text}
"""


def call_ai(prompt: str) -> str:
    """å‘¼å« Google Gemini API å–å¾—æ‘˜è¦ï¼ˆå…è²»æ–¹æ¡ˆï¼‰"""
    system = "ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡æ–°èç·¨è¼¯ã€‚"
    full_prompt = system + "\n\n" + prompt

    body = json.dumps({
        "contents": [{"parts": [{"text": full_prompt}]}],
        "generationConfig": {"maxOutputTokens": 2048},
    }).encode("utf-8")

    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key={GEMINI_API_KEY}"
    req = Request(
        url,
        data=body,
        headers={"Content-Type": "application/json"},
        method="POST",
    )

    with urlopen(req, timeout=60) as resp:
        data = json.loads(resp.read().decode())

    return data["candidates"][0]["content"]["parts"][0]["text"]


def send_telegram(text: str):
    """ç™¼é€è¨Šæ¯åˆ°æ‰€æœ‰ Telegram ç”¨æˆ¶"""
    url = f"https://api.telegram.org/bot{TELEGRAM_BOT_TOKEN}/sendMessage"

    for chat_id in TELEGRAM_CHAT_IDS:
        body = json.dumps({
            "chat_id": chat_id,
            "text": text,
            "parse_mode": "HTML",
            "disable_web_page_preview": True,
        }).encode("utf-8")

        req = Request(url, data=body, headers={"Content-Type": "application/json"}, method="POST")

        try:
            with urlopen(req, timeout=15) as resp:
                result = json.loads(resp.read().decode())
                if not result.get("ok"):
                    print(f"âš ï¸ Telegram ç™¼é€å¤±æ•— (chat_id: {chat_id}): {result}")
                else:
                    print(f"âœ… è¨Šæ¯å·²ç™¼é€åˆ° {chat_id}")
        except Exception as e:
            print(f"âš ï¸ Telegram ç™¼é€å¤±æ•— (chat_id: {chat_id}): {e}")


# â”€â”€â”€ ä¸»ç¨‹å¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main():
    print("ğŸ“¡ æ­£åœ¨æŠ“å–æ–°è...")
    seen = load_seen()
    print(f"ğŸ“‹ å·²è¨˜éŒ„ {len(seen)} å‰‡æ¨æ’­éçš„æ–°è")

    all_news = fetch_all_news(seen)

    total = sum(len(v) for v in all_news.values())
    print(f"ğŸ“° ä»Šå¤©å…±æŠ“å– {total} å‰‡æ–°èï¼ˆæœªæ¨æ’­éï¼‰")

    if total == 0:
        send_telegram("âš ï¸ ä»Šå¤©ç„¡æ³•æŠ“å–æ–°èï¼Œè«‹æª¢æŸ¥ RSS ä¾†æºã€‚")
        return

    print("ğŸ¤– æ­£åœ¨ç”¨ GPT-4o-mini ç”¢ç”Ÿæ‘˜è¦...")
    summary = call_ai(build_prompt(all_news))

    # Telegram è¨Šæ¯é•·åº¦é™åˆ¶ 4096 å­—
    if len(summary) > 4096:
        summary = summary[:4090] + "\n..."

    print("ğŸ“¤ æ­£åœ¨ç™¼é€åˆ° Telegram...")
    send_telegram(summary)

    # æ¨æ’­æˆåŠŸå¾Œæ‰è¨˜éŒ„ï¼ˆæ‰‹å‹•æ¸¬è©¦æ¨¡å¼ä¸è¨˜éŒ„ï¼Œé¿å…å½±éŸ¿æ˜å¤©çš„è‡ªå‹•æ¨æ’­ï¼‰
    if not IS_MANUAL:
        for items in all_news.values():
            mark_seen(items, seen)
        save_seen(seen)
        print(f"ğŸ’¾ å·²è¨˜éŒ„æœ¬æ¬¡æ¨æ’­æ¨™é¡Œï¼Œç¸½è¨ˆ {len(seen)} ç­†")
    else:
        print("â„¹ï¸ æ‰‹å‹•æ¸¬è©¦æ¨¡å¼ï¼Œä¸è¨˜éŒ„æ¨æ’­æ¨™é¡Œ")
    print("ğŸ‰ å®Œæˆï¼")


if __name__ == "__main__":
    main()
